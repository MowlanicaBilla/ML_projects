{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "source": [
        "# Required installations:\n",
        "# !pip install pandas numpy sqlalchemy pymongo chromadb sentence-transformers python-dotenv tqdm langchain openai langchain-community"
      ],
      "metadata": {
        "id": "gEzX4kowOmO7"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": 2,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "vSvOa2cPNSXX",
        "outputId": "f3849a24-b719-43b8-ad9a-1229dbdbe261"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/sentence_transformers/cross_encoder/CrossEncoder.py:13: TqdmExperimentalWarning: Using `tqdm.autonotebook.tqdm` in notebook mode. Use `tqdm.tqdm` instead to force console mode (e.g. in jupyter console)\n",
            "  from tqdm.autonotebook import tqdm, trange\n"
          ]
        }
      ],
      "source": [
        "## Importing required packages\n",
        "\n",
        "import json\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sqlalchemy import create_engine, Column, Integer, String, Float, DateTime, Text, text\n",
        "from sqlalchemy.exc import IntegrityError\n",
        "from sqlalchemy.orm.exc import FlushError\n",
        "from sqlalchemy.ext.declarative import declarative_base\n",
        "from sqlalchemy.orm import sessionmaker\n",
        "import chromadb\n",
        "from sentence_transformers import SentenceTransformer\n",
        "from tqdm import tqdm\n",
        "import logging\n",
        "from concurrent.futures import ThreadPoolExecutor\n",
        "from typing import List, Dict\n",
        "import os\n",
        "from dotenv import load_dotenv\n",
        "from langchain.llms import OpenAI\n",
        "from langchain.chains import LLMChain\n",
        "from langchain.prompts import PromptTemplate\n",
        "\n",
        "from google.colab import userdata"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## Setting up logging\n",
        "\n",
        "logging.basicConfig(\n",
        "    level=logging.INFO,\n",
        "    format='%(asctime)s - %(name)s - %(levelname)s - %(message)s',\n",
        "    handlers=[\n",
        "        logging.FileHandler('pipeline.log'),\n",
        "        logging.StreamHandler()\n",
        "    ]\n",
        ")\n",
        "logger = logging.getLogger(__name__)"
      ],
      "metadata": {
        "id": "TtvyRhJgO0Vd"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 1. Creating a dataset since a dataset was not provided"
      ],
      "metadata": {
        "id": "g5wovv_xNgff"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# 1. Sample Dataset Creation\n",
        "def create_sample_dataset(num_records=1000):\n",
        "    \"\"\"Create a sample dataset with mixed structured and unstructured data.\"\"\"\n",
        "\n",
        "    # Sample product categories and conditions\n",
        "    categories = ['Electronics', 'Books', 'Clothing', 'Home & Garden', 'Sports']\n",
        "    conditions = ['New', 'Like New', 'Good', 'Fair', 'Poor']\n",
        "\n",
        "    data = []\n",
        "    for i in range(num_records):\n",
        "        record = {\n",
        "            'id': i,\n",
        "            'title': f\"Product {i}\",\n",
        "            'category': np.random.choice(categories),\n",
        "            'price': round(np.random.uniform(10, 1000), 2),\n",
        "            'condition': np.random.choice(conditions),\n",
        "            'created_at': datetime.now().isoformat(),\n",
        "            'description': f\"This is a detailed description of product {i}. It includes various features and specifications.\",\n",
        "            'metadata': {\n",
        "                'seller_rating': round(np.random.uniform(1, 5), 1),\n",
        "                'views': np.random.randint(0, 1000),\n",
        "                'tags': np.random.choice(['premium', 'sale', 'featured'], size=np.random.randint(1, 3)).tolist()\n",
        "            }\n",
        "        }\n",
        "        data.append(record)\n",
        "\n",
        "    # Save as JSON\n",
        "    with open('product_data.json', 'w') as f:\n",
        "        json.dump(data, f)\n",
        "\n",
        "    # Save as CSV (flattened structure)\n",
        "    df = pd.json_normalize(data)\n",
        "    df.to_csv('product_data.csv', index=False)\n",
        "\n",
        "    return data"
      ],
      "metadata": {
        "id": "gbT_09a_O7qm"
      },
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 2. Database Schema and Setup"
      ],
      "metadata": {
        "id": "zc4gTRUnP1Cd"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "Base = declarative_base()\n",
        "\n",
        "class Product(Base):\n",
        "    __tablename__ = 'products'\n",
        "\n",
        "    id = Column(Integer, primary_key=True)\n",
        "    title = Column(String(255))\n",
        "    category = Column(String(100))\n",
        "    price = Column(Float)\n",
        "    condition = Column(String(50))\n",
        "    created_at = Column(DateTime)\n",
        "    description = Column(Text)\n",
        "    seller_rating = Column(Float)\n",
        "    views = Column(Integer)\n",
        "    tags = Column(String(255))  # Store as JSON string\n",
        "    embedding = Column(Text)    # Store as JSON string\n",
        "\n",
        "def setup_database():\n",
        "    \"\"\"Set up SQLite database with the defined schema.\"\"\"\n",
        "    engine = create_engine('sqlite:///products.db')\n",
        "    Base.metadata.create_all(engine)\n",
        "    return engine"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "b9y5wvq6P9A5",
        "outputId": "3d457d75-e757-4f54-9aa9-49bb08d4f90a"
      },
      "execution_count": 5,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-5-cb973449314c>:1: MovedIn20Warning: The ``declarative_base()`` function is now available as sqlalchemy.orm.declarative_base(). (deprecated since: 2.0) (Background on SQLAlchemy 2.0 at: https://sqlalche.me/e/b8d9)\n",
            "  Base = declarative_base()\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def ingest_data(data: list, engine):\n",
        "    \"\"\"Ingest data into the database.\"\"\"\n",
        "    try:\n",
        "        processed_data = preprocess_data(data)\n",
        "\n",
        "        # Create a session\n",
        "        Session = sessionmaker(bind=engine)\n",
        "        session = Session()\n",
        "\n",
        "        # Insert data\n",
        "        for record in tqdm(processed_data):\n",
        "            session.add(Product(**record))\n",
        "\n",
        "        session.commit()\n",
        "\n",
        "    except (IntegrityError, FlushError) as e:\n",
        "        print(\"Error during data ingestion:\", e)\n",
        "        session.rollback()\n",
        "        raise\n",
        "\n",
        "    finally:\n",
        "        session.close()"
      ],
      "metadata": {
        "id": "vSXMohAQSKkb"
      },
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 3. Data Preprocessing\n"
      ],
      "metadata": {
        "id": "T5hIH0PgP-eU"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def preprocess_data(data: list):\n",
        "    \"\"\"Preprocess the data before loading into database.\"\"\"\n",
        "    processed_data = []\n",
        "\n",
        "    for record in data:\n",
        "        processed_record = {\n",
        "            'id': record['id'],\n",
        "            'title': record['title'],\n",
        "            'category': record['category'],\n",
        "            'price': record['price'],\n",
        "            'condition': record['condition'],\n",
        "            'created_at': datetime.fromisoformat(record['created_at']),\n",
        "            'description': record['description'],\n",
        "            'seller_rating': record['metadata']['seller_rating'],\n",
        "            'views': record['metadata']['views'],\n",
        "            'tags': json.dumps(record['metadata']['tags'])\n",
        "        }\n",
        "        processed_data.append(processed_record)\n",
        "\n",
        "    return processed_data"
      ],
      "metadata": {
        "id": "8LdB4O9uQH_X"
      },
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 4. Vectorization\n"
      ],
      "metadata": {
        "id": "6tUBXHgiQJAC"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class VectorStore:\n",
        "    _instance = None  # Class variable to store the single instance\n",
        "\n",
        "    def __new__(cls):\n",
        "        \"\"\"Create a new instance only if one doesn't exist.\"\"\"\n",
        "        if not isinstance(cls._instance, cls):\n",
        "            cls._instance = super(VectorStore, cls).__new__(cls)\n",
        "            # Initialize ChromaDB client and collection here\n",
        "            cls._instance.model = SentenceTransformer('all-MiniLM-L6-v2')\n",
        "            cls._instance.chroma_client = chromadb.Client()\n",
        "            # Check if collection exists before creating\n",
        "            if \"product_embeddings\" not in cls._instance.chroma_client.list_collections():\n",
        "                cls._instance.collection = cls._instance.chroma_client.create_collection(\"product_embeddings\")\n",
        "            else:\n",
        "                # Get existing collection if it already exists\n",
        "                cls._instance.collection = cls._instance.chroma_client.get_collection(\"product_embeddings\")\n",
        "        return cls._instance\n",
        "\n",
        "    def generate_embedding(self, text: str) -> List[float]:\n",
        "        return self.model.encode(text).tolist()\n",
        "\n",
        "    def batch_generate_embeddings(self, texts: List[str], batch_size: int = 100) -> List[List[float]]:\n",
        "        return self.model.encode(texts, batch_size=batch_size).tolist()\n",
        "\n",
        "    def store_embeddings(self, ids: List[str], texts: List[str], embeddings: List[List[float]]):\n",
        "        self.collection.add(\n",
        "            ids=ids,\n",
        "            documents=texts,\n",
        "            embeddings=embeddings\n",
        "        )"
      ],
      "metadata": {
        "id": "xd78ld1JQRZG"
      },
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 5. Data Pipeline"
      ],
      "metadata": {
        "id": "NVC9Pkc-QWCV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class DataPipeline:\n",
        "    def __init__(self):\n",
        "        self.engine = setup_database()\n",
        "        self.vector_store = VectorStore()\n",
        "        Session = sessionmaker(bind=self.engine)\n",
        "        self.session = Session()\n",
        "\n",
        "    def ingest_data(self, data: List[Dict], batch_size: int = 100):\n",
        "        \"\"\"Ingest data into the database and vector store.\"\"\"\n",
        "        try:\n",
        "            logger.info(\"Starting data ingestion\")\n",
        "\n",
        "            # Preprocess data\n",
        "            processed_data = preprocess_data(data)\n",
        "\n",
        "            # Generate embeddings\n",
        "            texts = [f\"{record['title']} {record['description']}\" for record in processed_data]\n",
        "            embeddings = self.vector_store.batch_generate_embeddings(texts, batch_size)\n",
        "\n",
        "            # Store in database\n",
        "            for record, embedding in zip(processed_data, embeddings):\n",
        "                product = Product(**record, embedding=json.dumps(embedding))\n",
        "                self.session.add(product)\n",
        "\n",
        "            # Store in vector store\n",
        "            self.vector_store.store_embeddings(\n",
        "                ids=[str(record['id']) for record in processed_data],\n",
        "                texts=texts,\n",
        "                embeddings=embeddings\n",
        "            )\n",
        "\n",
        "            self.session.commit()\n",
        "            logger.info(\"Data ingestion completed successfully\")\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during data ingestion: {str(e)}\")\n",
        "            self.session.rollback()\n",
        "            raise\n",
        "\n",
        "    def query_similar(self, query_text: str, n_results: int = 5):\n",
        "        \"\"\"Query similar products based on text similarity.\"\"\"\n",
        "        try:\n",
        "            # Generate query embedding\n",
        "            query_embedding = self.vector_store.generate_embedding(query_text)\n",
        "\n",
        "            # Query vector store\n",
        "            results = self.vector_store.collection.query(\n",
        "                query_embeddings=[query_embedding],\n",
        "                n_results=n_results\n",
        "            )\n",
        "\n",
        "            # Fetch full records from database\n",
        "            product_ids = [int(id) for id in results['ids'][0]]\n",
        "            products = self.session.query(Product).filter(Product.id.in_(product_ids)).all()\n",
        "\n",
        "            return products\n",
        "\n",
        "        except Exception as e:\n",
        "            logger.error(f\"Error during similarity query: {str(e)}\")\n",
        "            raise"
      ],
      "metadata": {
        "id": "D2ipTR-5QVXt"
      },
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# 6. RAG Implementation"
      ],
      "metadata": {
        "id": "XYs1Vw8-QdW4"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "API_KEY = userdata.get('OPENAI_API_KEY')"
      ],
      "metadata": {
        "id": "X3g5yC79ZNDO"
      },
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class RAGSystem:\n",
        "    def __init__(self):\n",
        "        load_dotenv()\n",
        "        self.llm = OpenAI(api_key=API_KEY)\n",
        "        self.pipeline = DataPipeline()\n",
        "\n",
        "        self.prompt_template = PromptTemplate(\n",
        "            input_variables=[\"query\", \"context\"],\n",
        "            template=\"\"\"\n",
        "            Based on the following product information:\n",
        "            {context}\n",
        "\n",
        "            Answer the following question:\n",
        "            {query}\n",
        "            \"\"\"\n",
        "        )\n",
        "\n",
        "        self.chain = LLMChain(llm=self.llm, prompt=self.prompt_template)\n",
        "\n",
        "    def generate_response(self, query: str):\n",
        "        \"\"\"Generate a response using RAG.\"\"\"\n",
        "        # Retrieve relevant products\n",
        "        products = self.pipeline.query_similar(query)\n",
        "\n",
        "        # Format context\n",
        "        context = \"\\n\\n\".join([\n",
        "            f\"Product: {p.title}\\nCategory: {p.category}\\nPrice: ${p.price}\\nDescription: {p.description}\"\n",
        "            for p in products\n",
        "        ])\n",
        "\n",
        "        # Generate response\n",
        "        response = self.chain.run(query=query, context=context)\n",
        "        return response"
      ],
      "metadata": {
        "id": "AT_XWoQxQbff"
      },
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# usage\n",
        "if __name__ == \"__main__\":\n",
        "    # 1. Create sample dataset\n",
        "    data = create_sample_dataset(1000)\n",
        "    engine = setup_database()\n",
        "\n",
        "    # Clear existing data before ingesting new data\n",
        "    with engine.connect() as conn:\n",
        "        conn.execute(text(\"DELETE FROM products\"))\n",
        "        conn.commit()\n",
        "\n",
        "    # Remove the below line to avoid ingesting data twice\n",
        "    # ingest_data(data, engine)\n",
        "\n",
        "    # 2. Initialize and run pipeline\n",
        "    pipeline = DataPipeline()\n",
        "    pipeline.ingest_data(data)\n",
        "\n",
        "    # 3. Example query\n",
        "    results = pipeline.query_similar(\"electronic devices in new condition\")\n",
        "    for product in results:\n",
        "        print(f\"Title: {product.title}\")\n",
        "        print(f\"Category: {product.category}\")\n",
        "        print(f\"Price: ${product.price}\")\n",
        "        print(\"---\")\n",
        "\n",
        "    # 4. RAG example\n",
        "    rag_system = RAGSystem()\n",
        "    response = rag_system.generate_response(\"What are the best electronic products under $500?\")\n",
        "    print(\"RAG Response:\", response)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qaJgCcrNNewl",
        "outputId": "f46ae3ab-c225-4545-d917-e9e2357c4f67"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/huggingface_hub/utils/_auth.py:94: UserWarning: \n",
            "The secret `HF_TOKEN` does not exist in your Colab secrets.\n",
            "To authenticate with the Hugging Face Hub, create a token in your settings tab (https://huggingface.co/settings/tokens), set it as secret in your Google Colab and restart your session.\n",
            "You will be able to reuse this secret in all of your notebooks.\n",
            "Please note that authentication is recommended but still optional to access public models or datasets.\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Title: Product 1\n",
            "Category: Electronics\n",
            "Price: $270.07\n",
            "---\n",
            "Title: Product 51\n",
            "Category: Electronics\n",
            "Price: $917.34\n",
            "---\n",
            "Title: Product 202\n",
            "Category: Home & Garden\n",
            "Price: $12.9\n",
            "---\n",
            "Title: Product 203\n",
            "Category: Books\n",
            "Price: $888.93\n",
            "---\n",
            "Title: Product 518\n",
            "Category: Clothing\n",
            "Price: $968.28\n",
            "---\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-11-8f43f17fbd62>:4: LangChainDeprecationWarning: The class `OpenAI` was deprecated in LangChain 0.0.10 and will be removed in 1.0. An updated version of the class exists in the :class:`~langchain-openai package and should be used instead. To use it run `pip install -U :class:`~langchain-openai` and import as `from :class:`~langchain_openai import OpenAI``.\n",
            "  self.llm = OpenAI(api_key=API_KEY)\n",
            "<ipython-input-11-8f43f17fbd62>:18: LangChainDeprecationWarning: The class `LLMChain` was deprecated in LangChain 0.1.17 and will be removed in 1.0. Use :meth:`~RunnableSequence, e.g., `prompt | llm`` instead.\n",
            "  self.chain = LLMChain(llm=self.llm, prompt=self.prompt_template)\n",
            "<ipython-input-11-8f43f17fbd62>:32: LangChainDeprecationWarning: The method `Chain.run` was deprecated in langchain 0.1.0 and will be removed in 1.0. Use :meth:`~invoke` instead.\n",
            "  response = self.chain.run(query=query, context=context)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "RAG Response: \n",
            "Based on the given information, there are no electronic products under $500. The closest product to $500 is Product 400 which is priced at $778.32 and falls under the Electronics category. Therefore, there are no electronic products under $500 in this list. \n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Project Specification Document: Product Search and Q&A System\n",
        "\n",
        "**1. Introduction**\n",
        "\n",
        "This document outlines the specifications for a project that builds a product search and Q&A system using a combination of database management, vector embedding, and a large language model (LLM). The system will ingest product data, store it in a database and a vector store, allow users to search for similar products based on textual queries, and leverage a RAG (Retrieval Augmented Generation) approach to answer user questions about the products.\n",
        "\n",
        "**2. Project Goals**\n",
        "\n",
        "- **Efficient Product Search:** Enable users to quickly find relevant products based on text-based queries using semantic search.\n",
        "- **Contextualized Q&A:** Allow users to ask questions about the products and receive insightful answers based on the available product information.\n",
        "- **Scalable System:** Design a system that can handle a growing number of products and user queries effectively.\n",
        "\n",
        "**3. Project Steps & Rationale**\n",
        "\n",
        "**Step 1: Sample Dataset Creation ( `create_sample_dataset` )**\n",
        "\n",
        "* **Rationale:**  We start by creating a sample dataset since a dataset was not provided. This dataset has structured (e.g., price, category) and unstructured data (e.g., description) to simulate real-world product data. The dataset is saved in both JSON and CSV formats for flexibility.\n",
        "\n",
        "**Step 2: Database Schema and Setup ( `setup_database`, `Product` )**\n",
        "\n",
        "* **Rationale:** We define a database schema (SQLite in this case) to store our structured product data. The `Product` class represents the table structure for storing product details, including attributes like title, category, price, and embeddings. This ensures structured data is stored persistently and can be queried efficiently.\n",
        "\n",
        "**Step 3: Data Preprocessing ( `preprocess_data` )**\n",
        "\n",
        "* **Rationale:**  Before data is ingested, we need to preprocess it. This step ensures data consistency and prepares it for loading into the database.  Specifically, it formats the data into a format compatible with the database schema and removes unnecessary fields.\n",
        "\n",
        "**Step 4: Vectorization ( `VectorStore` )**\n",
        "\n",
        "* **Rationale:** We leverage a vector store (ChromaDB) to efficiently store and search through product embeddings. These embeddings are generated from the title and descriptions of the products, allowing for semantic understanding of text. The `VectorStore` class manages embedding generation using a pre-trained model (Sentence Transformer) and handling the interaction with ChromaDB.\n",
        "\n",
        "**Step 5: Data Pipeline ( `DataPipeline` )**\n",
        "\n",
        "* **Rationale:** We create a `DataPipeline` to streamline data ingestion. This class handles the ingestion of product data into the database and vector store. It ensures that the database and vector store are populated simultaneously with consistent information.\n",
        "  - it generates embedding and ingests them in a vector store\n",
        "  - It preprocesses data\n",
        "  - queries similar documents based on the query and embeddings in the vector store\n",
        "  - stores data into the database\n",
        "\n",
        "**Step 6: RAG Implementation ( `RAGSystem` )**\n",
        "\n",
        "* **Rationale:** The `RAGSystem` implements a retrieval augmented generation system. It uses the `DataPipeline` to retrieve relevant product information based on user queries. The retrieved information is formatted and presented to an LLM (OpenAI in this case). The LLM is prompted to answer the user's question based on the context retrieved from the database and vector store.\n",
        "  - Prompts LLM based on retrieved context.\n",
        "\n",
        "\n",
        "**4. Future Enhancements**\n",
        "\n",
        "- **Advanced Embedding Models:** Explore more powerful embedding models for improved semantic understanding.\n",
        "- **Data Enrichment:** Integrate additional data sources (e.g., reviews, images) to provide more comprehensive product information.\n",
        "- **User Feedback:** Implement mechanisms for collecting user feedback on search results and Q&A responses.\n",
        "- **Deployment:** Deploy the system on a cloud platform to make it accessible to a wider audience.\n",
        "\n",
        "**5. Conclusion**\n",
        "\n",
        "This project specification outlines the key aspects of developing a product search and Q&A system. By combining database management, vector embedding, and LLMs, we can create a system that effectively assists users in finding relevant product information and answering their questions in a insightful way.\n",
        "\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "eUxa3mqzcBZN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "q96uJ8RpcADY"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}